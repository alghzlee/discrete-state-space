{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle   #打开pkl包\n",
    "import numpy as np\n",
    "# from ismember import ismember\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import os\n",
    "gamma = 0.9\n",
    "device='cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionalDQN(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, N_ATOMS):\n",
    "        super(DistributionalDQN, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(state_dim, 128)\n",
    "        self.hiddens = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()) for _ in range(7)])\n",
    "\n",
    "        self.out = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, state):\n",
    "        batch_size = state.size()[0]\n",
    "        out = self.input_layer(state)\n",
    "        for layer in self.hiddens:\n",
    "            out = layer(out)\n",
    "\n",
    "        out = self.out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dist_DQN(object):\n",
    "    def __init__(self,\n",
    "                 state_dim=37,\n",
    "                 num_actions=25,\n",
    "                 v_max=20,\n",
    "                 v_min=-20,\n",
    "                 device='cpu',\n",
    "                 gamma=0.9,\n",
    "                 tau=0.005,\n",
    "                 n_atoms=51\n",
    "                 ):\n",
    "        self.device=device\n",
    "\n",
    "        self.Q = DistributionalDQN(state_dim, num_actions, n_atoms).to(self.device)\n",
    "        self.Q_target = copy.deepcopy(self.Q)\n",
    "        self.optimizer = torch.optim.Adam(self.Q.parameters(), lr=0.000005)  #0.00001\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.atoms = n_atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self,batchs,epoch):\n",
    "\n",
    "        (state, next_state, action, next_action, reward, done,bloc_num)=batchs\n",
    "        states_num = state.shape[0]\n",
    "        batch_s = 128\n",
    "        uids = np.unique(bloc_num)\n",
    "        num_batch = uids.shape[0] // batch_s  # 分批次\n",
    "        record_loss_num = 0\n",
    "        record_loss = []\n",
    "\n",
    "        sum_q_loss = 0\n",
    "        Batch = 0\n",
    "        for batch_idx in range(num_batch):\n",
    "            batch_uids = uids[batch_idx * batch_s: (batch_idx + 1) * batch_s]\n",
    "            batch_user = np.isin(bloc_num, batch_uids)\n",
    "            state_user = state[batch_user, :]\n",
    "            next_state_user = next_state[batch_user, :]\n",
    "            action_user = action[batch_user]\n",
    "            next_action_user = next_action[batch_user]\n",
    "            reward_user = reward[batch_user]\n",
    "            done_user = done[batch_user]\n",
    "            batch = (state_user, next_state_user, action_user, next_action_user,reward_user, done_user)\n",
    "            loss = self.compute_loss(batch)\n",
    "            sum_q_loss += loss.item()\n",
    "            if Batch % 25 == 0:\n",
    "                print('Epoch :', epoch, 'Batch :', Batch, 'Average Loss :', sum_q_loss / (Batch + 1))\n",
    "                record_loss1 = sum_q_loss / (Batch + 1)\n",
    "                record_loss.append(record_loss1)\n",
    "\n",
    "            self.optimizer.zero_grad()  #梯度清零\n",
    "            loss.backward()             #反向传播\n",
    "            self.optimizer.step()       #更新\n",
    "\n",
    "            #更新Q_target网络参数\n",
    "            if num_batch%50==0:\n",
    "                self.polyak_target_update()\n",
    "\n",
    "            Batch += 1\n",
    "\n",
    "        return record_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak_target_update(self):    #更新网络\n",
    "        for param, target_param in zip(self.Q.parameters(), self.Q_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self, batch):\n",
    "        state, next_state, action, next_action, reward, done = batch\n",
    "        batch_size = state.shape[0]\n",
    "        range_batch = torch.arange(batch_size).long().to(device)\n",
    "\n",
    "        #利用神经网络输出动作\n",
    "        log_Q_dist_prediction = self.Q(state)\n",
    "        log_Q_dist_prediction1 = log_Q_dist_prediction[range_batch, action]   #  1810  一个一维的数\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Q_dist_target= self.Q_target(next_state)\n",
    "\n",
    "        #求最大值\n",
    "        a_star = torch.argmax(Q_dist_target, dim=1)\n",
    "\n",
    "        log_Q_experience = Q_dist_target[range_batch, next_action.squeeze(1)]\n",
    "\n",
    "        #最大的Q值\n",
    "        Q_dist_star = Q_dist_target[range_batch, a_star]\n",
    "\n",
    "        # 更新 targetQ Q值=============================\n",
    "        end_multiplier = 1 - done\n",
    "        eplion=0.2\n",
    "\n",
    "        targetQ = reward + (gamma * end_multiplier* (Q_dist_star+eplion*(log_Q_experience-Q_dist_star)))\n",
    "\n",
    "        td_error = torch.square(targetQ - log_Q_dist_prediction1)\n",
    "        old_loss = torch.mean(td_error)\n",
    "\n",
    "        return old_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            batch_size = state.shape[0]\n",
    "            Q_dist= self.Q(state)\n",
    "            a_star = torch.argmax(Q_dist, dim=1)\n",
    "            return a_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
